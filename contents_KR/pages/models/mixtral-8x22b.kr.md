# Mixtral 8x22B

Mixtral 8x22B는 Mistral AI가 출시한 새로운 오픈 대형 언어 모델(LLM)입니다. Mixtral 8x22B는 총 141B 매개변수 중 39B 활성 매개변수를 가진 희소 전문가 혼합 모델로 특징지어집니다.

## 능력(Capabilities)

Mixtral 8x22B는 다국어 이해, 수학 추론, 코드 생성, 네이티브 함수 호출 지원, 제약된 출력 지원을 포함하는 능력을 가진 비용 효율적인 모델로 훈련되었습니다. 모델은 64K 토큰의 컨텍스트 윈도우 크기를 지원하여 대용량 문서에서 고성능 정보 회상을 가능하게 합니다.

Mistral AI는 Mixtral 8x22B가 최고의 성능 대비 비용 비율 커뮤니티 모델 중 하나를 제공하며 희소 활성화로 인해 상당히 빠르다고 주장합니다.

*출처: [Mistral AI Blog](https://mistral.ai/news/mixtral-8x22b/)*

## 결과(Results)

[공식 보고된 결과](https://mistral.ai/news/mixtral-8x22b/)에 따르면, Mixtral 8x22B(39B 활성 매개변수)는 MMLU, HellaS, TriQA, NaturalQA 등 여러 추론 및 지식 벤치마크에서 Command R+와 Llama 2 70B와 같은 최첨단 오픈 모델을 능가합니다.

*출처: [Mistral AI Blog](https://mistral.ai/news/mixtral-8x22b/)*

Mixtral 8x22B는 GSM8K, HumanEval, Math와 같은 벤치마크에서 평가될 때 코딩 및 수학 작업에서 모든 오픈 모델을 능가합니다. Mixtral 8x22B Instruct가 GSM8K에서 90% 점수(maj@8)를 달성한다고 보고됩니다.

*출처: [Mistral AI Blog](https://mistral.ai/news/mixtral-8x22b/)*

Mixtral 8x22B에 대한 더 많은 정보와 사용 방법은 여기서 확인할 수 있습니다: https://docs.mistral.ai/getting-started/open_weight_models/#operation/listModels

모델은 Apache 2.0 라이선스 하에 출시됩니다. 