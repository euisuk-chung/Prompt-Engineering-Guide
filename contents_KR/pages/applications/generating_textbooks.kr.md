# 생성 데이터셋 다양성 확보(Tackling Generated Datasets Diversity)

이전 [챕터](https://www.promptingguide.ai/applications/synthetic_rag)에서는 LLM(대형 언어 모델)을 활용해 합성 데이터셋을 생성하고, 이를 로컬 리트리버(Retriever) 모델 파인튜닝에 활용하는 가능성을 다뤘습니다. 이 방법은 대규모 비라벨 문서 코퍼스가 있을 때 각 문서에서 하나 이상의 합성 쿼리를 생성해 쿼리-문서 쌍을 만드는 방식입니다.

하지만 정보 검색(Information Retrieval)이 아닌 과제라면 어떨까요? 예를 들어, 법률 문서 분류 문제를 다루는데 외부 API로 데이터를 전송할 수 없다면, 로컬 모델을 직접 학습시켜야 합니다. 이때 데이터 수집이 큰 장애물이 되어 제품 개발이 지연될 수 있습니다.

간단한 예로, 동화 생성이 목표라고 가정해봅시다. 이는 [Eldan 등(2023)](https://arxiv.org/abs/2305.07759)의 연구 출발점이기도 합니다. 각 동화는 2~3개의 단락으로 구성되며, 단순한 플롯과 주제를 따르고, 전체 데이터셋은 아동의 어휘와 사실적 지식을 포괄합니다.

언어는 단순한 규칙과 기호 체계가 아니라 의미를 전달·해석하는 수단입니다. LLM으로 학습 데이터를 생성할 때 가장 큰 과제는 데이터셋의 다양성(diversity) 확보입니다. [생성 온도(temperature)](https://www.promptingguide.ai/introduction/settings)를 높여도, 모델이 반복적이고 다양성이 부족한 데이터셋을 생성하는 경우가 많습니다(아동 언어조차도). 일관성(coherence)과 관련성(relevance) 역시 자연어 생성의 주요 도전 과제입니다.

이 문제를 해결하기 위해, 저자들은 약 1,500개의 기본 단어(아동 어휘)를 명사, 동사, 형용사로 분류해 준비했습니다. 각 생성 시, 동사 1개, 명사 1개, 형용사 1개를 무작위로 선택해 프롬프트에 포함시켜 이야기를 생성합니다.

이 기법은 데이터셋의 다양성을 크게 확장해, 아동 어휘 전체와 다양한 개념 조합을 포괄하는 동화를 생성할 수 있게 했습니다. 또한, 대화, 반전, 나쁜 결말, 교훈 등 다양한 스토리 특성(feature)도 랜덤하게 포함시켜 프롬프트에 명시했습니다.

*프롬프트 예시:*
```
Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would likely understand. The story should use the verb ”{random.choice(verbs_list)}”, the noun ”{random.choice(nouns_list)}” and the adjective ”{random.choice(adjectives_list)}”. The story should have the following features: {random.choice(features_list)}, {random.choice(features_list)}. Remember to only use simple words!
```

예를 들어, 다음과 같이 프롬프트를 채울 수 있습니다:

*프롬프트 예시:*
```
Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would likely understand. The story should use the verb ”decorate”, the noun ”thunder” and the adjective ”ancient”. The story should have the following features: the story should contain at least one dialogue, the story has a bad ending. Remember to only use simple words!
```

*출력 예시:*
```
Once upon a time, in an ancient house, there lived a girl named Lily. She loved to decorate her room with pretty things. One day, she found a big box in the attic. She opened it and saw many shiny decorations. Lily was very happy and decided to use them in her room. As Lily was decorating her room, the sky outside became dark. There was a loud thunder sound, and Lily got scared. She ran to her mom and said, ”Mommy, the thunder is so loud!” Her mom hugged her and said, ”Don’t worry, it will pass soon.” But the thunder did not stop. It got louder and louder, and the ancient house started to shake. Suddenly, the roof fell down on the pretty decorations. Lily was sad because her room was not pretty anymore. The end.
```

이처럼 최신 텍스트 생성 모델(GPT-3.5, GPT-4 등)은 지침에 따라 대량의 합성 콘텐츠를 생성할 수 있습니다. 프롬프트가 매번 무작위로 구성되고, 모델이 이를 정확히 따르기 때문에 이야기는 매우 다양하게 생성됩니다. 한두 단어만 같아도 완전히 다른 이야기가 만들어집니다. 즉, 프롬프트에 무작위성을 주입해 다양한 데이터셋을 생성하는 방식입니다.

핵심 절차는 다음과 같습니다:

1. 합성 데이터셋 샘플 간 변동 가능한 파라미터/엔티티 식별
2. 이 엔티티들을 수집·정리해 빈칸 채우기용 풀(pool)로 활용
3. 엔티티를 무작위로 선택해 데이터셋 생성(생성 온도는 기본값보다 높게, 최대값보다는 낮게 설정 권장)
4. ChatGPT/GPT-4의 생성 결과로 로컬 모델 학습

여기서 엔티티 중 하나는 클래스 레이블이 될 수도 있습니다. 예를 들어, 감정 분류 과제에서는 프롬프트에 "positive" 또는 "negative"를 직접 명시해 해당 레이블의 텍스트를 생성하고, 이를 로컬 분류기 학습에 활용할 수 있습니다.

# 반복적(계층적) 합성 데이터 생성(Iterative (Hierarchical) Synthetic Data Generation)

엔티티 수를 늘리거나 일부 엔티티를 LLM이 사전 생성하도록 하여, 더 복잡한 프롬프트 구조도 가능합니다. 예를 들어, 먼저 이야기 요약(summary)와 반드시 포함되어야 할 문장(sentence)을 생성한 뒤, 이를 최종 프롬프트에 삽입해 이야기를 생성할 수 있습니다.

*프롬프트 예시:*
```
Summary: {a short summary generated by LLM, using the approach above}
Features: {copy the features from the initial prompt}
Sentence: {a sentence generated by LLM, which should be present in the story}
Words: {copy the words from the initial prompt}
Story:
```

예시:

*프롬프트 예시:*
```
Summary: Lily and Timmy build a sandcastle together and learn to compromise, but it gets knocked over by a gust of wind. They find beauty in the broken sandcastle and play happily with a butterfly.
Features: Dialogue, Foreshadowing, Twist
Sentence: One day, she went to the park and saw a beautiful butterfly.
Words: disagree, network, beautiful
Story:
```

*출력 예시:*
```
Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, she went to the park and saw a beautiful butterfly. She tried to catch it, but it flew away. Lily’s friend, Timmy, came to play with her. They decided to build a sandcastle, but they disagreed on how to build it. Lily wanted to make it tall, while Timmy wanted to make it wide. They argued for a while, but finally agreed to make it both tall and wide. As they were building the sandcastle, they noticed a group of ants working together. They realized that the ants were building a network of tunnels. Suddenly, a gust of wind came and knocked over their sandcastle. They were sad, but then they saw the butterfly again. It landed on the remains of their sandcastle and they realized that it was still beautiful, even in its broken state. They smiled and played together happily.
```

이처럼 수십만 개의 다양한 예시를 생성해 모델 학습에 활용할 수 있습니다. 예를 들어, 텍스트에 대화(dialogue)나 반전(plot twist)이 포함되어 있는지 분류하는 분류기를 학습하려면, 프롬프트에 레이블을 명시해 생성된 샘플별로 타깃 값을 알 수 있습니다.

# 교과서만 있으면 된다(Textbooks Are All You Need)

이 접근법에서 중요한 질문은, 합성 데이터셋이 실제 응용에서 네트워크 학습에 진정한 이점을 제공할 수 있는가입니다. 다행히도, 저자들은 이 질문에 대한 실험을 통해 SOTA LLM에서 생성한 합성 데이터로 소형 언어 모델을 학습시켜 그 효과를 검증했습니다.

[Gunasekar 등(2023)](https://arxiv.org/abs/2306.11644)은 고품질 학습 데이터의 중요성을 강조하며, 언어 모델이 "교과서"와 같은 특성(명확성, 포괄성, 정보성, 편향 없음)을 갖춘 자료로 학습될 때 더 효과적이라고 주장합니다.

이 원칙을 바탕으로, 저자들은 LLM(Phi-1) 학습을 위한 준합성(semi-synthetic) 데이터셋을 제작했습니다. 주요 평가 과제는 주어진 텍스트 설명 또는 독스트링(docstring)에 따라 파이썬 함수를 생성하는 것입니다. 모델 품질 평가는 HumanEval 벤치마크([Chen 등, 2021](https://arxiv.org/abs/2107.03374))로 진행됩니다.

이 접근법에서 다양성은 여러 이유로 매우 중요합니다. 다양한 코딩 표현과 문제 해결 방식을 모델에 노출시켜 과적합(overfitting)이나 특정 패턴 의존을 줄이고, 새로운 과제에 대한 적응력을 높일 수 있습니다.

코드 생성 과제에서는 다음과 같은 제약을 두고 교과서 스타일의 문서를 생성했습니다:
- 주제(topics)
- 타깃 독자(target audience)

아쉽게도, 합성 데이터 생성에 사용된 프롬프트 템플릿은 공개되지 않았으나, 결과물은 논문에서 확인할 수 있습니다. 저자들은 GPT-4 대신 ChatGPT(GPT-3.5)를 사용했으며, 이 전략만으로도 합성 데이터로 파인튜닝 시 우수한 결과를 얻었습니다.

![생성된 교과서 스타일 샘플](../../img/synthetic_diversity/textbooks_1.png)

이미지 출처: [Gunasekar 등(2023)](https://arxiv.org/abs/2306.11644)

특히 타깃 독자 제약에 주목해야 합니다. 고등학생, 학부생, 박사과정 등 대상에 따라 설명 방식과 정보 처리 방식이 달라지므로, 데이터 다양성을 높일 수 있습니다. 아래는 프롬프트 재현 예시입니다.

*프롬프트 예시:*
```
Write an extract from a Computer Science textbook for a 1st-year bachelor. The coding language is Python 3.6.

This is an extract from the middle of the following topic: Singular matrices.

The extract starts with a high-level overview of the topic. Then, it presents an example and describes the solution in natural language. After that, it provides 1-2 code snippets, following the example. Each snippet has no more than 10 rows. There should be no text after code snippets.

Keep in mind that the extract should be concise and explain only one concept at a time.  The code should represent a Python function & its call. All the common ML/DS libraries are available.

Don't be too verbose. The expected length of the extract is 2 paragraphs at most.
```

*출력 예시:*
```
**Singular Matrices**

In linear algebra, a matrix is termed as "singular" if it doesn't have an inverse. Another equivalent definition is when the determinant of the matrix is zero. Singular matrices are crucial because they represent systems of linear equations that do not have a unique solution.

Consider a 2x2 matrix `[[a, b], [c, d]]`. If the product of `ad` and `bc` is the same (i.e., `ad = bc`), then this matrix is singular. Let's explore this with an example: given the matrix `[[2, 4], [1, 2]]`, you'll find that the product of the diagonals yields the same result (2 * 2 = 4 * 1), thus it's a singular matrix.

import numpy as np

def is_singular(matrix):
    determinant = np.linalg.det(matrix)
    return abs(determinant) < 1e-9

matrix_example = np.array([[2, 4], [1, 2]])
print(is_singular(matrix_example))  # returns True
```

실제로 저자들은 총 10억(1B) 토큰을 생성해 모델 학습에 활용했고, 이를 통해 1.5B 파라미터 소형 모델이 10배 더 큰 모델과 경쟁할 수 있었습니다(자세한 내용은 [Gunasekar 등(2023)](https://arxiv.org/abs/2306.11644) 참고).

![Phi-1 성능, 대형 모델과 비교](../../img/synthetic_diversity/textbooks_2.png)

이미지 출처: [Gunasekar 등(2023)](https://arxiv.org/abs/2306.11644)

실제 업무에서는 이 정도 대규모 합성 데이터가 필요하지 않을 수 있습니다(논문은 사전학습용 대규모 데이터 기준). 참고로, 1k 토큰당 $0.002(표준 ChatGPT 요금) 기준, 10억 토큰 생성에는 약 $2000, 프롬프트 비용도 비슷하게 소요됩니다.

합성 데이터 파인튜닝은 도메인이 특수할수록(특히 비영어권 등) 더 큰 효과를 보입니다. 또한, [연쇄적 사고(CoT)](https://www.promptingguide.ai/techniques/cot) 등 다양한 프롬프트 기법과도 잘 결합됩니다. 오픈소스 모델(Alpaca, Vicuna 등)도 합성 데이터 파인튜닝으로 뛰어난 성능을 보이고 있습니다. 