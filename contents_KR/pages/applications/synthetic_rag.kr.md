# RAG를 위한 합성 데이터셋 생성(Generating Synthetic Dataset for RAG)

## RAG 설정을 위한 합성 데이터(Synthetic Data for RAG Setup)
안타깝게도, 머신러닝 엔지니어의 삶에서는 종종 라벨링된 데이터가 부족하거나 매우 적습니다. 일반적으로 이를 깨닫게 되면 프로젝트는 데이터 수집과 라벨링의 긴 과정을 시작합니다. 몇 달 후에야 솔루션 개발을 시작할 수 있습니다.

그러나 LLM의 출현으로 일부 제품에서 패러다임이 바뀌었습니다: 이제 LLM의 일반화 능력에 의존하여 아이디어를 테스트하거나 AI 기반 기능을 거의 즉시 개발할 수 있습니다. 의도한 대로 (거의) 작동한다면 전통적인 개발 프로세스를 시작할 수 있습니다.

이미지 출처: [The Rise of the AI Engineer, by S. Wang](https://www.latent.space/p/ai-engineer)

새로운 접근 방식 중 하나는 [검색 증강 생성(Retrieval Augmented Generation, RAG)](https://www.promptingguide.ai/techniques/rag)입니다. 모델의 지식만으로는 의존할 수 없는 지식 집약적 작업에 사용됩니다. RAG는 정보 검색 구성 요소와 텍스트 생성 모델을 결합합니다. 이 접근 방식에 대해 자세히 알아보려면 [가이드의 관련 섹션](https://www.promptingguide.ai/techniques/rag)을 참조하세요.

RAG의 핵심 구성 요소는 관련 문서를 식별하고 LLM에 전달하여 추가 처리를 하는 검색 모델입니다. 검색 모델의 성능이 좋을수록 제품이나 기능 결과가 좋아집니다. 이상적으로는 검색이 즉시 잘 작동합니다. 그러나 성능은 종종 다른 언어나 특정 도메인에서 떨어집니다.

이를 상상해보세요: 체코 법률과 법적 관행에 기반하여 질문에 답변하는 챗봇을 만들어야 합니다(물론 체코어로). 또는 인도 시장에 맞춤화된 세금 어시스턴트(GPT-4 발표 중 OpenAI가 제시한 사용 사례)를 설계해야 합니다. 검색 모델이 가장 관련성 높은 문서를 놓치고 전반적으로 성능이 좋지 않아 시스템의 품질을 제한한다는 것을 발견할 가능성이 높습니다.

하지만 해결책이 있습니다. 새로운 트렌드는 기존 LLM을 사용하여 새로운 세대의 LLM/검색기/기타 모델 훈련을 위한 데이터를 합성하는 것입니다. 이 프로세스는 프롬프트 기반 쿼리 생성을 통해 LLM을 표준 크기 인코더로 증류하는 것으로 볼 수 있습니다. 증류는 계산 집약적이지만 추론 비용을 상당히 줄이고 특히 저자원 언어나 특수 도메인에서 성능을 크게 향상시킬 수 있습니다.

이 가이드에서는 지시사항에 따라 방대한 양의 합성 콘텐츠를 생성할 수 있는 ChatGPT와 GPT-4와 같은 최신 텍스트 생성 모델에 의존할 것입니다. [Dai et al. (2022)](https://arxiv.org/abs/2209.11755)는 단 8개의 수동 라벨링된 예시와 라벨링되지 않은 대용량 데이터(검색용 문서, 예: 모든 파싱된 법률)만으로 거의 최첨단 성능을 달성할 수 있는 방법을 제안했습니다. 이 연구는 합성 생성 데이터가 데이터 부족으로 인해 지도 도메인 내 미세조정이 어려운 작업에 대한 작업별 검색기 훈련을 촉진한다는 것을 확인합니다.

## 도메인별 데이터셋 생성(Domain-Specific Dataset Generation)
LLM을 활용하려면 짧은 설명을 제공하고 몇 가지 예시를 수동으로 라벨링해야 합니다. 다른 검색 작업이 다양한 검색 의도를 가지고 있다는 점, 즉 "관련성"의 다른 정의를 가지고 있다는 점에 주목하는 것이 중요합니다. 다시 말해, 동일한 (쿼리, 문서) 쌍에 대해 관련성이 검색 의도에 따라 완전히 다를 수 있습니다. 예를 들어, 논증 검색 작업은 지지 논증을 찾을 수 있지만 다른 작업은 반대 논증을 요구합니다([ArguAna 데이터셋](https://aclanthology.org/P18-1023/)에서 볼 수 있듯이).

아래 예시를 고려해보세요. 이해를 쉽게 하기 위해 영어로 작성되었지만, ChatGPT/GPT-4가 저자원 언어도 효율적으로 처리하므로 데이터는 어떤 언어든 될 수 있다는 것을 기억하세요.

*프롬프트:*
```
작업: 주어진 논증에 대한 반대 논증을 식별하세요.

논증 #1: {여기에 구절 X1 삽입}

논증 #1과 관련된 간결한 반대 논증 쿼리: {여기에 수동으로 준비된 쿼리 Y1 삽입}

논증 #2: {여기에 구절 X2 삽입}
논증 #2와 관련된 간결한 반대 논증 쿼리: {여기에 수동으로 준비된 쿼리 Y2 삽입}

<- 여기에 예시를 붙여넣으세요 ->

논증 N: 벌금을 소득에 비례하게 만들더라도 원하는 영향의 평등을 얻을 수 없습니다. 이는 영향이 단순히 소득에 비례하지 않고 다른 여러 요소를 고려해야 하기 때문입니다. 예를 들어, 가족을 부양하는 사람은 처분 가능한 소득이 적기 때문에 그렇지 않은 사람보다 더 큰 영향을 받을 것입니다. 또한 소득 기반 벌금은 전체 부(즉, 누군가가 실제로 가지고 있는 돈의 양: 누군가는 많은 자산을 가질 수 있지만 높은 소득을 가지지 않을 수 있음)를 무시합니다. 제안은 이러한 불평등을 고려하지 않으며, 이는 훨씬 더 큰 왜곡 효과를 가질 수 있으므로 논증이 일관되지 않게 적용되고 있습니다.

논증 #N과 관련된 간결한 반대 논증 쿼리:
```

*출력:*
```
벌금을 소득에 비례하게 만들면 처벌이 상대적이 될 것입니다
```

일반적으로 이러한 프롬프트는 다음과 같이 표현할 수 있습니다:

$(e_{prompt}, e_{doc}(d_{1}), e_{query}(q_1), . . . , e_{doc}(d_k), e_{query}(q_k), e_{doc}(d))$

여기서 $e_{doc}$와 $e_{query}$는 각각 작업별 문서, 쿼리 설명이고, $e_{prompt}$는 ChatGPT/GPT-4를 위한 작업별 프롬프트/지시사항이며, $d$는 LLM이 쿼리를 생성할 새로운 문서입니다.

이 프롬프트에서 마지막 문서 $d$와 생성된 쿼리만 로컬 모델의 추가 훈련에 사용됩니다. 이 접근 방식은 대상 검색 코퍼스 $D$가 사용 가능하지만 새로운 작업에 대한 주석이 달린 쿼리-문서 쌍의 수가 제한적일 때 적용할 수 있습니다.

전체 파이프라인 개요:

이미지 출처: [Dai et al. (2022)](https://arxiv.org/abs/2209.11755)

예시의 수동 주석을 책임감 있게 처리하는 것이 중요합니다. 더 많은 것(예: 20개)을 준비하고 그 중 2-8개를 무작위로 선택하여 프롬프트에 포함하는 것이 좋습니다. 이는 주석 작업의 상당한 시간 비용 없이 생성된 데이터의 다양성을 증가시킵니다. 그러나 이러한 예시는 대표적이고, 올바르게 형식화되어야 하며, 대상 쿼리 길이나 톤과 같은 세부사항까지 포함해야 합니다. 예시와 지시사항이 더 정확할수록 검색기 훈련을 위한 합성 데이터가 더 좋아집니다. 품질이 낮은 퓨샷 예시는 훈련된 모델의 결과 품질에 부정적인 영향을 미칠 수 있습니다.

대부분의 경우 ChatGPT와 같은 더 저렴한 모델을 사용하는 것으로 충분합니다. 영어가 아닌 특이한 도메인과 언어에서도 잘 작동하기 때문입니다. 지시사항과 4-5개 예시가 포함된 프롬프트는 일반적으로 700개 토큰을 차지하고(검색기 제약으로 인해 각 구절이 128개 토큰을 초과하지 않는다고 가정) 생성은 25개 토큰입니다. 따라서 로컬 모델 미세조정을 위한 50,000개 문서 코퍼스에 대한 합성 데이터셋 생성 비용은 다음과 같습니다: `50,000 * (700 * 0.001 * $0.0015 + 25 * 0.001 * $0.002) = 55`, 여기서 `$0.0015`와 `$0.002`는 GPT-3.5 Turbo API에서 1,000개 토큰당 비용입니다. 동일한 문서에 대해 2-4개의 쿼리 예시를 생성하는 것도 가능합니다. 그러나 종종 추가 훈련의 이점이 가치가 있으며, 특히 일반 도메인(영어 뉴스 검색과 같은)이 아닌 특정 도메인(언급된 체코 법률과 같은)에 대해 검색기를 사용하는 경우에 그렇습니다.

50,000이라는 숫자는 무작위가 아닙니다. [Dai et al. (2022)](https://arxiv.org/abs/2209.11755)의 연구에서 이것이 합성 데이터로 훈련된 모델의 품질과 일치하기 위해 필요한 수동 라벨링된 데이터의 대략적인 수라고 명시되어 있습니다. 제품을 출시하기 전에 최소 10,000개의 예시를 수집해야 한다고 상상해보세요! 최소 한 달이 걸리고 노동 비용은 확실히 천 달러를 초과할 것이며, 이는 합성 데이터 생성과 로컬 검색기 모델 훈련보다 훨씬 많습니다. 이제 오늘 배운 기술로 단 며칠 만에 두 자릿수 지표 성장을 달성할 수 있습니다!

이미지 출처: [Dai et al. (2022)](https://arxiv.org/abs/2209.11755)

그리고 여기 BeIR 벤치마크의 일부 데이터셋에 대한 동일한 논문의 프롬프트 템플릿이 있습니다.

이미지 출처: [Dai et al. (2022)](https://arxiv.org/abs/2209.11755) 